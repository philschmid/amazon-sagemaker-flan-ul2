{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy FLAN-UL2 20B on Amazon SageMaker\n",
    "\n",
    "Welcome to this Amazon SageMaker guide on how to deploy the [FLAN-UL2 20B](https://huggingface.co/google/flan-ul2) on Amazon SageMaker for inference. We will deploy [philschmid/flan-ul2-20b-fp16](https://huggingface.co/philschmid/flan-ul2-20b-fp16) to Amazon SageMaker for real-time inference using Hugging Face Inference Deep Learning Container.\n",
    "\n",
    "![flan-t5-on-amazon-sagemaker](./assets/sagemaker-endpoint.png)\n",
    "\n",
    "What we are going to do \n",
    "1. Create FLAN-T5 XXL inference script with bnb quantization\n",
    "2. Create SageMaker `model.tar.gz` artifact\n",
    "3. Deploy the model to Amazon SageMaker\n",
    "4. Run inference using the deployed model\n",
    "\n",
    "\n",
    "## Quick intro: FLAN-UL2, a bigger FLAN-T5\n",
    "\n",
    "Flan-UL2 is an encoder decoder (seq2seq) model based on the T5 architecture. It uses the same configuration as the UL2 model released earlier last year. It was fine tuned using the \"Flan\" prompt tuning and dataset collection. FLAN-UL2 was trained as part of the [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) paper. Noticeable difference to FLAN-T5 XXL are: \n",
    "\n",
    "* FLAN-UL2 has context window of 2048 compared to 512 for FLAN-T5 XXL\n",
    "* +~3% better performance than FLAN-T5 XXL on [benchmarks](https://huggingface.co/google/flan-ul2#performance-improvment)\n",
    "\n",
    "\n",
    "![flan-ul2](./assets/flan.webp)\n",
    "\n",
    "- Paper: https://arxiv.org/abs/2210.11416\n",
    "- Official repo: https://github.com/google-research/t5x\n",
    "\n",
    "--- \n",
    "\n",
    "Before we can get started we have to install the missing dependencies to be able to create our `model.tar.gz` artifact and create our Amazon SageMaker endpoint. \n",
    "We also have to make sure we have the permission to create our SageMaker Endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 \"huggingface_hub==0.13.0\" \"hf-transfer\" --upgrade\n",
    "!pip install git+https://github.com/aws/sagemaker-python-sdk.git\n",
    "# !pip install \"sagemaker>=2.140.0\" boto3 \"huggingface_hub==0.13.0\" \"hf-transfer\" --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c22e8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create FLAN-UL2 20B inference script with bnb quantization\n",
    "\n",
    "Amazon SageMaker allows us to customize the inference script by providing a `inference.py` file. The `inference.py` file is the entry point to our model. It is responsible for loading the model and handling the inference request. If you are used to deploying Hugging Face Transformers that might be new to you. Usually, we just provide the `HF_MODEL_ID` and `HF_TASK` and the Hugging Face DLC takes care of the rest. For `FLAN-T5-XXL` thats not yet possible. We have to provide the `inference.py` file and implement the `model_fn` and `predict_fn` functions to efficiently load the 11B large model. \n",
    "\n",
    "If you want to learn more about creating a custom inference script you can check out [Creating document embeddings with Hugging Face's Transformers & Amazon SageMaker](https://www.philschmid.de/custom-inference-huggingface-sagemaker)\n",
    "\n",
    "In addition to the `inference.py` file we also have to provide a `requirements.txt` file. The `requirements.txt` file is used to install the dependencies for our `inference.py` file.\n",
    "\n",
    "The first step is to create a `code/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "435a574f",
   "metadata": {},
   "source": [
    "As next we create a `requirements.txt` file and add the `accelerate` and `bitsandbytes` library to it. The `accelerate` library is used efficiently to load the model on multiple GPUs. The `bitsandbytes` library is used to quantize the model to 8bit using LLM.int8(). LLM.int8 introduces a new quantization technique for Int8 matrix multiplication, which cuts the memory needed for inference by half while. To learn more about check out this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration) or the [paper](https://arxiv.org/abs/2208.07339)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf302de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "accelerate==0.17.1\n",
    "transformers==4.25.1\n",
    "bitsandbytes==0.37.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c24c2ca9",
   "metadata": {},
   "source": [
    "The last step for our inference handler is to create the `inference.py` file. The `inference.py` file is responsible for loading the model and handling the inference request. The `model_fn` function is called when the model is loaded. The `predict_fn` function is called when we want to do inference. \n",
    "\n",
    "We are using the `AutoModelForSeq2SeqLM` class from transformers load the model from the local directory (`model_dir`) in the `model_fn`. In the `predict_fn` function we are using the `generate` function from transformers to generate the text for a given input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "from typing import Dict, List, Any\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load model and processor from model_dir\n",
    "    model =  AutoModelForSeq2SeqLM.from_pretrained(model_dir, device_map=\"auto\", load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # unpack model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    # process input\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "\n",
    "    # preprocess\n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # pass inputs with all kwargs in data\n",
    "    if parameters is not None:\n",
    "        outputs = model.generate(input_ids, **parameters)\n",
    "    else:\n",
    "        outputs = model.generate(input_ids)\n",
    "\n",
    "    # postprocess the prediction\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return [{\"generated_text\": prediction}]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## Create SageMaker `model.tar.gz` artifact\n",
    "\n",
    "To use our `inference.py` we need to bundle it together with our model weights into a `model.tar.gz`. The archive includes all our model-artifcats to run inference. The `inference.py` script will be placed into a `code/` folder. We will use the `huggingface_hub` SDK to easily download [philschmid/flan-ul2-20b-fp16](https://huggingface.co/philschmid/flan-ul2-20b-fp16) from [Hugging Face](hphilschmid/flan-ul2-20b-fp16) and then upload it to Amazon S3 with the `sagemaker` SDK. The model `philschmid/flan-ul2-20b-fp16` is a sharded fp16 version of the [google/flan-ul2](https://huggingface.co/google/flan-ul2)\n",
    "\n",
    "Make sure the enviornment has enough diskspace to store the model, ~35GB should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5105d2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flan-ul2/code/requirements.txt', 'flan-ul2/code/inference.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# set HF_HUB_ENABLE_HF_TRANSFER env var to enable hf-transfer for faster downloads\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID=\"google/flan-ul2\"\n",
    "# create model dir\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "model_tar_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "# snapshot_download(HF_MODEL_ID, local_dir=str(model_tar_dir), local_dir_use_symlinks=False)\n",
    "\n",
    "# copy code/ to model dir\n",
    "copy_tree(\"code/\", str(model_tar_dir.joinpath(\"code\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6080d38a",
   "metadata": {},
   "source": [
    "Before we can upload the model to Amazon S3 we have to create a `model.tar.gz` archive. Important is that the archive should directly contain all files and not a folder with the files. For example, your file should look like this:\n",
    "\n",
    "```\n",
    "model.tar.gz/\n",
    "|- config.json\n",
    "|- pytorch_model-00001-of-00012.bin\n",
    "|- tokenizer.json\n",
    "|- ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5705e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir=os.getcwd()\n",
    "# change to model dir\n",
    "os.chdir(str(model_tar_dir))\n",
    "# use pigz for faster and parallel compression\n",
    "!tar -cf model.tar.gz --use-compress-program=pigz * \n",
    "# change back to parent dir\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbce144",
   "metadata": {},
   "source": [
    "After we created the `model.tar.gz` archive we can upload it to Amazon S3. We will use the `sagemaker` SDK to upload the model to our sagemaker session bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175511f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model uploaded to: s3://sagemaker-us-east-1-558105141721/flan-ul2/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri = S3Uploader.upload(local_path=str(model_tar_dir.joinpath(\"model.tar.gz\")), desired_s3_uri=f\"s3://{sess.default_bucket()}/flan-ul2\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "## Deploy the model to Amazon SageMaker\n",
    "\n",
    "After we have uploaded our model archive we can deploy our model to Amazon SageMaker. We will use `HuggingfaceModel` to create our real-time inference endpoint.\n",
    "\n",
    "We are going to deploy the model to an `g5.12xlarge` instance. The `g5.12xlarge` instance is a GPU instance with 4x NVIDIA A10G GPU. If you are interested in how you could add autoscaling to your endpoint you can check out [Going Production: Auto-scaling Hugging Face Transformers with Amazon SageMaker](https://www.philschmid.de/auto-scaling-sagemaker-huggingface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.26\",  # transformers version used\n",
    "   pytorch_version=\"1.13\",       # pytorch version used\n",
    "   py_version='py39',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "   #  container_startup_health_check_timeout=600, # increase timeout for large models\n",
    "   #  model_data_download_timeout=600, # increase timeout for large models\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6b3812f",
   "metadata": {},
   "source": [
    "## Run inference using the deployed model\n",
    "\n",
    "The `.deploy()` returns an `HuggingFacePredictor` object which can be used to request inference using the `.predict()` method. Our endpoint expects a `json` with at least `inputs` key. \n",
    "\n",
    "When using generative models, most of the time you want to configure or customize your prediction to fit your needs, for example by using beam search, configuring the max or min length of the generated sequence, or adjusting the temperature to reduce repetition.\n",
    "The Transformers library provides different strategies and kwargs to do this, the Hugging Face Inference toolkit offers the same functionality using the parameters attribute of your request payload. Below you can find examples on how to generate text without parameters, with beam search, and using custom configurations. If you want to learn about different decoding strategies check out this [blog post](https://huggingface.co/blog/how-to-generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c5366b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\\"topk_cpu\\\" not implemented for \\u0027Half\\u0027\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-03-16-09-47-10-381 in account 558105141721 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m parameters \u001b[39m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mdo_sample\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mmax_new_tokens\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m50\u001b[39m,\n\u001b[1;32m     10\u001b[0m   \u001b[39m# \"top_p\": 0.95,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[39m# Run prediction\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m predictor\u001b[39m.\u001b[39;49mpredict({\n\u001b[1;32m     15\u001b[0m \t\u001b[39m\"\u001b[39;49m\u001b[39minputs\u001b[39;49m\u001b[39m\"\u001b[39;49m: payload,\n\u001b[1;32m     16\u001b[0m   \u001b[39m\"\u001b[39;49m\u001b[39mparameters\u001b[39;49m\u001b[39m\"\u001b[39;49m :parameters\n\u001b[1;32m     17\u001b[0m })\n\u001b[1;32m     18\u001b[0m \u001b[39m# [{'generated_text': 'Peter stayed with Elizabeth at the hospital for 3 days.'}]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/predictor.py:161\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m        as is.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_request_args(\n\u001b[1;32m    159\u001b[0m     data, initial_args, target_model, target_variant, inference_id\n\u001b[1;32m    160\u001b[0m )\n\u001b[0;32m--> 161\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49msagemaker_runtime_client\u001b[39m.\u001b[39;49minvoke_endpoint(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_args)\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/botocore/client.py:960\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    959\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 960\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    961\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\\"topk_cpu\\\" not implemented for \\u0027Half\\u0027\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-03-16-09-47-10-381 in account 558105141721 for more information."
     ]
    }
   ],
   "source": [
    "payload = \"\"\"Summarize the following text: \n",
    "Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.\n",
    "Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well. \n",
    "Therefore, Peter stayed with her at the hospital for 3 days without leaving.\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "  \"do_sample\": True,\n",
    "  \"max_new_tokens\": 50,\n",
    "  # \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "# Run prediction\n",
    "predictor.predict({\n",
    "\t\"inputs\": payload,\n",
    "  \"parameters\" :parameters\n",
    "})\n",
    "# [{'generated_text': 'Peter stayed with Elizabeth at the hospital for 3 days.'}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00bddf6a",
   "metadata": {},
   "source": [
    "Lets try another examples! This time we focus ond questions answering with a step by step approach including some simple math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"\"\"Answer the following question step by step:\n",
    "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. \n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "  \"early_stopping\": True,\n",
    "  \"length_penalty\": 2.0,\n",
    "  \"max_new_tokens\": 50,\n",
    "  \"temperature\": 0,\n",
    "}\n",
    "\n",
    "# Run prediction\n",
    "predictor.predict({\n",
    "\t\"inputs\": payload,\n",
    "  \"parameters\" :parameters\n",
    "})\n",
    "# [{'generated_text': 'He buys 2 cans of tennis balls, so he has 2 * 3 = 6 tennis balls. He has 5 + 6 = 11 tennis balls now.'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10007d",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6fb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776934c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a40944fb6d302ad2eace17cfbb714ee95a1e6c7ab311709595ca70171602490b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
